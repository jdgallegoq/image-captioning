{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Image Captioning\n",
        "This notebook is a hands-on lab provided by Google.\n",
        "\n",
        "In this notebook an image captioning model is going to be trained using visual attention mechanism.\n",
        "\n",
        "The main goal is to generate text based on an image as input. This text will describe as approximately as possible the content of the image."
      ],
      "metadata": {
        "id": "ldvMoYTVaHC2"
      },
      "id": "ldvMoYTVaHC2"
    },
    {
      "cell_type": "code",
      "id": "GexHPBfFRn7JI1sCL9tHm3El",
      "metadata": {
        "tags": [],
        "id": "GexHPBfFRn7JI1sCL9tHm3El",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8f7b0c5-bdfb-461c-b596-60bf81de40d7"
      },
      "source": [
        "import time\n",
        "from textwrap import wrap\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras.layers import (\n",
        "    GRU,\n",
        "    Add,\n",
        "    AdditiveAttention,\n",
        "    Attention,\n",
        "    Concatenate,\n",
        "    Dense,\n",
        "    Embedding,\n",
        "    LayerNormalization,\n",
        "    Reshape,\n",
        "    StringLookup,\n",
        "    TextVectorization,\n",
        ")\n",
        "\n",
        "print(tf.version.VERSION)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.17.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read data.\n",
        "* **Dataset:** COCO captions.\n",
        "* **Feature extractor:** `InceptionResNetV2`."
      ],
      "metadata": {
        "id": "BFGuK7Nkbsp-"
      },
      "id": "BFGuK7Nkbsp-"
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 20000 # this can be change to control accuracy/speed\n",
        "ATTENTION_DIM = 512\n",
        "WORD_EMBEDDING_DIM = 128\n",
        "\n",
        "FEATURE_EXTRACTOR = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\"\n",
        ")\n",
        "IMG_HEIGHT = 299\n",
        "IMG_WIDTH = 299\n",
        "IMG_CHANNELS = 3\n",
        "FEATURE_SHAPE = (8, 8, 1523) # as inception_resnet_v2 feature shape"
      ],
      "metadata": {
        "id": "jUYXfXdlb7gw"
      },
      "id": "jUYXfXdlb7gw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter and preprocess\n",
        "* Resize images to defined shape above\n",
        "* Rescale pixel values for speed up process\n",
        "* Return images as `image_tensor` and `captions` dictionary"
      ],
      "metadata": {
        "id": "DujGA_FJcsph"
      },
      "id": "DujGA_FJcsph"
    },
    {
      "cell_type": "code",
      "source": [
        "GCS_DIR = \"gs://asl-public/data/tensorflow_datasets/\"\n",
        "BUFFER_SIZE = 1000\n",
        "\n",
        "def get_image_label(example):\n",
        "  caption = example[\"captions\"][\"text\"][0] # only first caption per image\n",
        "  img = example[\"image\"]\n",
        "  img = tf.image.resize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
        "  img = img/255\n",
        "  return {\"image_tensor\": img, \"caption\": caption}\n",
        "\n",
        "trainds = tfds.load(\"coco_captions\", split=\"train\", data_dir=GCS_DIR)\n",
        "\n",
        "trainds = trainds.map(\n",
        "    get_image_label,\n",
        "    num_parallel_calls=tf.data.AUTOTUNE\n",
        ").shuffle(BUFFER_SIZE)\n",
        "trainds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "AUSwjnMub7jo"
      },
      "id": "AUSwjnMub7jo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize examples"
      ],
      "metadata": {
        "id": "LclYKT4ldk18"
      },
      "id": "LclYKT4ldk18"
    },
    {
      "cell_type": "code",
      "source": [
        "f, ax = plt.subplots(1, 4, figsize=(20, 5))\n",
        "for idx, data in enumerate(trainds.take(4)):\n",
        "  ax[idx].imshow(data[\"image_tensor\"].numpy())\n",
        "  caption = \"\\n\".join(wrap(data[\"caption\"].numpy().decode(\"utf-8\"), 30))\n",
        "  ax[idx].set_title(caption)\n",
        "  ax[idx].axis(\"off\")"
      ],
      "metadata": {
        "id": "89WX-8B0b7mJ"
      },
      "id": "89WX-8B0b7mJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text preprocessing\n",
        "Define special tokens `<start>` and `<end>`"
      ],
      "metadata": {
        "id": "VXt9XwPDd9g2"
      },
      "id": "VXt9XwPDd9g2"
    },
    {
      "cell_type": "code",
      "source": [
        "def add_start_end_token(data):\n",
        "  start = tf.convert_to_tensor(\"<start>\")\n",
        "  end = tf.convert_to_tensor(\"<end>\")\n",
        "  data[\"caption\"] = tf.strings.join(\n",
        "      [start, data[\"caption\"], end], separator=\" \"\n",
        "  )\n",
        "  return data"
      ],
      "metadata": {
        "id": "CuBG__SGb7oo"
      },
      "id": "CuBG__SGb7oo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_CAPTION_LEN = 64 # this can be defined by getting some descriptives on captions lenght\n",
        "\n",
        "def standardize(inputs):\n",
        "  inputs = tf.strings.lower(inputs)\n",
        "  return tf.strings.regex_replace(\n",
        "      inputs, r\"[!\\\"#$%&\\(\\)\\*\\+.,-/:;=?@\\[\\\\\\]^_`{|}~]?\", \"\"\n",
        "  )\n",
        "\n",
        "tokenizer = TextVectorization(\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    standardize=standardize,\n",
        "    output_sequence_length=MAX_CAPTION_LEN\n",
        ")\n",
        "\n",
        "tokenizer.adapt(trainds.map(lambda data: data[\"caption\"]))"
      ],
      "metadata": {
        "id": "H2oEjVzGfmyp"
      },
      "id": "H2oEjVzGfmyp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test tokenizer\n",
        "tokenizer([\"<start> This is a sentence <end>\"])"
      ],
      "metadata": {
        "id": "j3Butalkfmpg"
      },
      "id": "j3Butalkfmpg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_captions = []\n",
        "for d in trainds.take(5):\n",
        "  sample_captions.append(d[\"caption\"].numpy())"
      ],
      "metadata": {
        "id": "Q4eVihXogvOX"
      },
      "id": "Q4eVihXogvOX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see captions with the standarizing tokens\n",
        "print(sample_captions)\n",
        "# see how coul it be tokenized\n",
        "tokenizer(sample_captions[:2])"
      ],
      "metadata": {
        "id": "UgxYPapSb7rI"
      },
      "id": "UgxYPapSb7rI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if want to do the inverse process\n",
        "for wordid in tokenizer(sample_captions[0])[0]:\n",
        "  print(tokenizer.get_vocabulary()[wordid], end=\" \")"
      ],
      "metadata": {
        "id": "-BdO5XXJhP2f"
      },
      "id": "-BdO5XXJhP2f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create word to index converters\n",
        "word_to_index = StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary()\n",
        ")\n",
        "index_to_word = StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary(),\n",
        "    invert=True\n",
        ")"
      ],
      "metadata": {
        "id": "vhahQofHAvW8"
      },
      "id": "vhahQofHAvW8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create training dataset\n",
        "Need to have targets in format `\"I love cats <end> <padding>\"` instead of `\"<start> I love cats <end>\"`"
      ],
      "metadata": {
        "id": "VrQo76WgBrCy"
      },
      "id": "VrQo76WgBrCy"
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "def create_ds_fn(data):\n",
        "  img_tensor = data[\"image_tensor\"]\n",
        "  caption = data[\"caption\"]\n",
        "  target = tf.roll(caption, -1, 0) # here the first word rolls to first position\n",
        "  zeros = tf.zeros([1], dtype=tf.int64)\n",
        "  target = tf.concat([target[:-1], zeros], axis=-1)\n",
        "  return (img_tensor, caption), target"
      ],
      "metadata": {
        "id": "TFqzwkG0AvUB"
      },
      "id": "TFqzwkG0AvUB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batched_ds = (\n",
        "    trainds.map(create_ds_fn)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "2Bklv0wIAvRH"
      },
      "id": "2Bklv0wIAvRH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see how it'll be\n",
        "for (img, caption), label in batched_ds.take(2):\n",
        "    print(f\"Image shape: {img.shape}\")\n",
        "    print(f\"Caption shape: {caption.shape}\")\n",
        "    print(f\"Label shape: {label.shape}\")\n",
        "    print(caption[0])\n",
        "    print(label[0])"
      ],
      "metadata": {
        "id": "sGDu3JX3AvJn"
      },
      "id": "sGDu3JX3AvJn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "### The Image encoder\n",
        "1. Extract features with `InceptionResNetV2`.\n",
        "2. Reshape vector to (Batch size, 64, 1536)\n",
        "3. Squash it to a lenght of `ATTENTION_DIM` with a Dense Layer and return (Batch Size, 64 `ATTENTION_DIM`)\n",
        "4. The attention layer attends over the image to predict the next word."
      ],
      "metadata": {
        "id": "5D71EcvbEQZK"
      },
      "id": "5D71EcvbEQZK"
    },
    {
      "cell_type": "code",
      "source": [
        "FEATURE_EXTRACTOR.trainable = False\n",
        "\n",
        "image_input = Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "image_features = FEATURE_EXTRACTOR(image_input)\n",
        "\n",
        "x = Reshape(\n",
        "    (FEATURE_SHAPE[0] * FEATURE_SHAPE[1], FEATURE_SHAPE[2])\n",
        ")(img_features)\n",
        "encoder_output = Dense(ATTENTION_DIM, activation=\"relu\")(x)"
      ],
      "metadata": {
        "id": "njCTnbmXF04j"
      },
      "id": "njCTnbmXF04j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = tf.keras.Model(inputs=image_input, outputs=encoder_output)\n",
        "encoder.summary()"
      ],
      "metadata": {
        "id": "pYf9qNANFzlZ"
      },
      "id": "pYf9qNANFzlZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Caption decoder\n",
        "1. Receives a word tokens batch\n",
        "2. Embeds the word tokens to `ATTENTION_DIM` dimension\n",
        "3. Pass it to GRU. Returns GRU outputs and states\n",
        "4. Bahdanau-style attention attends over the encoder's output feature by using GRU output as query\n",
        "5. Performs an skip connection using GRU (step 3) output and attention's output as well, then these are normalized\n",
        "6. Generates logit preds for next token"
      ],
      "metadata": {
        "id": "sdVTEUvfIKHz"
      },
      "id": "sdVTEUvfIKHz"
    },
    {
      "cell_type": "code",
      "source": [
        "# 1\n",
        "word_input = Input(shape=(MAX_CAPTION_LEN,), name=\"words\")\n",
        "# 2\n",
        "embed_x = Embedding(VOCAB_SIZE, ATTENTION_DIM)(word_input)\n",
        "# 3\n",
        "decoder_gru = GRU(\n",
        "    ATTENTION_DIM,\n",
        "    return_sequences=True,\n",
        "    return_state=True,\n",
        "    name=\"gru\"\n",
        ")\n",
        "gru_output, gru_state = decoder_gru(embed_x)\n",
        "# 4\n",
        "decoder_attention = Attention()\n",
        "context_vector = decoder_attention([gru_output, encoder_output])\n",
        "# 5\n",
        "addition = Add()([gru_output, context_vector])\n",
        "layer_norm = LayerNormalization(axis=-1)\n",
        "layer_norm_out = layer_norm(addition)\n",
        "# 6\n",
        "decoder_output_dense = Dense(VOCAB_SIZE)\n",
        "decoder_output = decoder_output_dense(layer_norm_out)"
      ],
      "metadata": {
        "id": "WrwRHXQqJdvD"
      },
      "id": "WrwRHXQqJdvD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = tf.keras.Model(\n",
        "    inputs=[word_input, encoder_output],\n",
        "    outputs=decoder_output\n",
        ")\n",
        "tf.keras.utils.plot_model(decoder)"
      ],
      "metadata": {
        "id": "YVh4LMTsKkxj"
      },
      "id": "YVh4LMTsKkxj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder.summary()"
      ],
      "metadata": {
        "id": "WQmfRg3NK1mp"
      },
      "id": "WQmfRg3NK1mp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model training"
      ],
      "metadata": {
        "id": "-EJzgTzmL8YH"
      },
      "id": "-EJzgTzmL8YH"
    },
    {
      "cell_type": "code",
      "source": [
        "# define one single model that compiles\n",
        "image_caption_train_model = tf.keras.Model(\n",
        "    inputs=[image_input, word_input],\n",
        "    outputs=decoder_output\n",
        ")\n",
        "\n",
        "# define a loss function to be a cross-entropy\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction=\"none\"\n",
        ")\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  loss_ = loss_object(real, pred)\n",
        "  # returns 1 to word index and 0 to padding\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  mask = tf.cast(mask, dtype=tf.int32)\n",
        "  sentence_len = tf.reduce_sum(mask)\n",
        "  loss_ = loss_[:sentence_len]\n",
        "\n",
        "  return tf.reduce_mean(loss_, 1)"
      ],
      "metadata": {
        "id": "F1Mx1FrSL9QN"
      },
      "id": "F1Mx1FrSL9QN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_caption_train_model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=loss_function,\n",
        ")"
      ],
      "metadata": {
        "id": "-ozrN1EmNFnW"
      },
      "id": "-ozrN1EmNFnW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = image_caption_train_model.fit(batched_ds, epoch=1)"
      ],
      "metadata": {
        "id": "wOKGPsbTNnim"
      },
      "id": "wOKGPsbTNnim",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_state_input = Input(shape=(ATTENTION_DIM,), name=\"gru_state_input\")\n",
        "\n",
        "gru_output, gru_state = decoder_gru(embed_x, initial_state=gru_state_input)\n",
        "\n",
        "context_vector = decoder_attention([gru_output, encoder_output])\n",
        "addition_output = Add()([gru_output, context_vector])\n",
        "layer_norm_output = layer_norm(addition_output)\n",
        "\n",
        "decoder_output = decoder_output_dense(layer_norm_output)\n",
        "\n",
        "# define prediction omdel with state input and output\n",
        "decoder_pred_model = tf.keras.Model(\n",
        "    inputs=[word_input, gru_state_input, encoder_output],\n",
        "    otuputs=[decoder_output, gru_state],\n",
        ")"
      ],
      "metadata": {
        "id": "1kxC1BoRUyGS"
      },
      "id": "1kxC1BoRUyGS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict captions"
      ],
      "metadata": {
        "id": "qqa-x1CkXGgp"
      },
      "id": "qqa-x1CkXGgp"
    },
    {
      "cell_type": "code",
      "source": [
        "MINIMUM_SENTENCE_LENGHT = 5\n",
        "\n",
        "def predict_caption(filename):\n",
        "  gru_state = tf.zeros((1, ATTENTION_DIM))\n",
        "\n",
        "  img = tf.image.decode_jpeg(tf.io.read_file(filename), channels=IMG_CHANNELS)\n",
        "  img = img / 255\n",
        "\n",
        "  features = encoder(tf.expand_dims(img, axis=0))\n",
        "  dec_input = tf.expand_dims([word_to_index(\"<start>\")], 1)\n",
        "  result = []\n",
        "  for i in range(MAX_CAPTION_LEN):\n",
        "    predictions, gru_state = decoder_pred_model(\n",
        "        [dec_input, gru_state, features],\n",
        "      )\n",
        "    top_probs, top_idxs = tf.math.top_k(\n",
        "        input=predictions[0][0],\n",
        "        k=10,\n",
        "        sorted=False\n",
        "    )\n",
        "    chosen_id = tf.random.categorical([top_probs], 1)[0].numpy()\n",
        "    predicted_id = top_idx.numpy()[chosen_id][0]\n",
        "\n",
        "    result.append(tokenizer.get_vocabulary()[predicted_id])\n",
        "\n",
        "    if predicted_id == word_to_index(\"<end>\"):\n",
        "      return img, result\n",
        "\n",
        "    dec_input = tf.expand_dims([predicted_id], 1)\n",
        "\n",
        "  return img, result"
      ],
      "metadata": {
        "id": "FLnyBbTeUyB5"
      },
      "id": "FLnyBbTeUyB5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test it\n",
        "filename = \"../sample_images/baseball.jpeg\"\n",
        "\n",
        "for i in range(5):\n",
        "  image, caption = predict_caption(filename)\n",
        "  print(\" \".join(caption[:-1]) + \".\")\n",
        "\n",
        "img = tf.image.decode_jpeg(tf.io.read_file(filename), channels=IMG_CHANNELS)\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x041CPsmUx_N"
      },
      "id": "x041CPsmUx_N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w-CeHPzjUnxO"
      },
      "id": "w-CeHPzjUnxO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "image_captioning"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}